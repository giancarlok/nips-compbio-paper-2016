{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn \n",
    "import scipy\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold, train_test_split, cross_val_score, StratifiedKFold, LabelKFold, ShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mhcflurry import peptide_encoding, amino_acid\n",
    "from mhcflurry.amino_acid import common_amino_acids\n",
    "from mhcflurry import dataset\n",
    "from mhcflurry.dataset import Dataset\n",
    "import matplotlib.pyplot as plt \n",
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import math \n",
    "import statsmodels.api as sm\n",
    "from keras import models, layers, optimizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, Convolution1D, AveragePooling1D, Activation, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.engine import topology\n",
    "import seaborn as sns\n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionLSTM(LSTM):\n",
    "    def __init__(self, output_dim, attention_vec, **kwargs):\n",
    "        self.attention_vec = attention_vec\n",
    "        super(AttentionLSTM, self).__init__(output_dim, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(AttentionLSTM, self).build(input_shape)\n",
    "\n",
    "        assert hasattr(self.attention_vec, '_keras_shape')\n",
    "        attention_dim = self.attention_vec._keras_shape[1]\n",
    "\n",
    "        self.U_a = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                   name='{}_U_a'.format(self.name))\n",
    "        self.b_a = K.zeros((self.output_dim,), name='{}_b_a'.format(self.name))\n",
    "\n",
    "        self.U_m = self.inner_init((attention_dim, self.output_dim),\n",
    "                                   name='{}_U_m'.format(self.name))\n",
    "        self.b_m = K.zeros((self.output_dim,), name='{}_b_m'.format(self.name))\n",
    "\n",
    "        self.U_s = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                   name='{}_U_s'.format(self.name))\n",
    "        self.b_s = K.zeros((self.output_dim,), name='{}_b_s'.format(self.name))\n",
    "\n",
    "        self.trainable_weights += [self.U_a, self.U_m, self.U_s,\n",
    "                                   self.b_a, self.b_m, self.b_s]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def step(self, x, states):\n",
    "        h, [h, c] = super(AttentionLSTM, self).step(x, states)\n",
    "        attention = states[4]\n",
    "\n",
    "        m = K.tanh(K.dot(h, self.U_a) + attention + self.b_a)\n",
    "        s = K.exp(K.dot(m, self.U_s) + self.b_s)\n",
    "        h = h * s\n",
    "\n",
    "        return h, [h, c]\n",
    "\n",
    "    def get_constants(self, x):\n",
    "        constants = super(AttentionLSTM, self).get_constants(x)\n",
    "        constants.append(K.dot(self.attention_vec, self.U_m) + self.b_m)\n",
    "        return constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"bdata.2009.mhci.public.1.txt\")\n",
    "\n",
    "df['log_meas']=1-np.log(df['meas'])/math.log(50000)\n",
    "df['peptide_length'] = df['sequence'].str.len()\n",
    "\n",
    "\n",
    "max_len=df['sequence'].str.len().max()\n",
    "n_peptides = df['sequence'].count()\n",
    "\n",
    "def amino_acid_hotshot_encoding(s):\n",
    "    return common_amino_acids.hotshot_encoding([s],len(s)).flatten().astype(int)\n",
    "df['hotshot_encoded_peptides'] = df.sequence.apply(lambda seq: amino_acid_hotshot_encoding(seq))\n",
    "\n",
    "def amino_acid_index_encoding(s, maxlen):\n",
    "    a = 1+common_amino_acids.index_encoding([s],len(s)).flatten()\n",
    "    return np.concatenate([a, np.zeros(maxlen-len(a),dtype=int)])\n",
    "df['index_encoded_peptides'] = df.sequence.apply(lambda seq: amino_acid_index_encoding(seq, max_len))\n",
    "\n",
    "def measured_affinity_less_than(Y,k):\n",
    "    IC50 = 50000**(1-Y)\n",
    "    return (IC50 < k).astype(int) \n",
    "\n",
    "def affinity_label(Y):\n",
    "    return measured_affinity_less_than(Y,50) + measured_affinity_less_than(Y,500) + measured_affinity_less_than(Y,5000) + measured_affinity_less_than(Y,50000)\n",
    "\n",
    "df['affinity_label'] = affinity_label(df['log_meas'])\n",
    "df_h = df[df['mhc']=='HLA-A-0201'][['hotshot_encoded_peptides','index_encoded_peptides','log_meas','peptide_length']]\n",
    "X = np.array(list(df_h['index_encoded_peptides']))\n",
    "y = np.array(list(df_h['log_meas']))\n",
    "y[y<0]=0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folds =3 \n",
    "n_epochs = 5\n",
    "\n",
    "train_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "nine_train_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "nine_test_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "non_nine_train_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "non_nine_test_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_attentional_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "test_attentional_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "nine_train_attentional_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "nine_test_attentional_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "non_nine_train_attentional_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "non_nine_test_attentional_lstm_aucs = np.zeros((folds,n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size_nn = 16\n",
    "batch_size_lstm = 16\n",
    "hidden = 50\n",
    "dropout_probability = 0.25\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(KFold(len(df_h),folds, shuffle=True)):\n",
    "    \n",
    "    # normal LSTM\n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm = Model(input = sequence, output = output)\n",
    "    adam = Adam(lr = 0.01)\n",
    "    lstm.compile(optimizer = adam , loss='mean_squared_error')\n",
    "    \n",
    "    # attentional LSTM\n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = AttentionLSTM(hidden, Input(shape=(32,)) )(embedded)\n",
    "    backwards = AttentionLSTM(hidden,Input(shape=(32,)), go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    attentional_lstm = Model(input = sequence, output = output)\n",
    "    attentional_adam = Adam(lr = 0.01)\n",
    "    attentional_lstm.compile(optimizer = adam , loss='mean_squared_error')\n",
    "    \n",
    "    # index sets\n",
    "    \n",
    "    nine_train_idx = np.array([i for i in train_idx if (np.count_nonzero(X[i])==9)])\n",
    "    non_nine_train_idx = np.array([i for i in train_idx if (np.count_nonzero(X[i])!=9)])\n",
    "    nine_test_idx = np.array([i for i in test_idx if (np.count_nonzero(X[i])==9)])\n",
    "    non_nine_test_idx = np.array([i for i in test_idx if (np.count_nonzero(X[i])!=9)])\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        adam.lr.set_value(0.01*(epoch+1)**(-2))\n",
    "        lstm.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm.predict(X[train_idx]))\n",
    "        test_lstm_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm.predict(X[test_idx]))\n",
    "        nine_train_lstm_auc = roc_auc_score(measured_affinity_less_than(y[nine_train_idx],500),lstm.predict(X[nine_train_idx]))\n",
    "        nine_test_lstm_auc = roc_auc_score(measured_affinity_less_than(y[nine_test_idx],500),lstm.predict(X[nine_test_idx]))\n",
    "        non_nine_train_lstm_auc = roc_auc_score(measured_affinity_less_than(y[non_nine_train_idx],500),lstm.predict(X[non_nine_train_idx]))\n",
    "        non_nine_test_lstm_auc = roc_auc_score(measured_affinity_less_than(y[non_nine_test_idx],500),lstm.predict(X[non_nine_test_idx]))\n",
    "        \n",
    "        train_lstm_aucs[i][epoch]=train_lstm_auc\n",
    "        test_lstm_aucs[i][epoch]=test_lstm_auc\n",
    "        nine_train_lstm_aucs[i][epoch]=nine_train_lstm_auc\n",
    "        nine_test_lstm_aucs[i][epoch]=nine_test_lstm_auc\n",
    "        non_nine_train_lstm_aucs[i][epoch]=non_nine_train_lstm_auc\n",
    "        non_nine_test_lstm_aucs[i][epoch]=non_nine_test_lstm_auc\n",
    "        \n",
    "        \n",
    "        attentional_adam.lr.set_value(0.01*(epoch+1)**(-2))\n",
    "        attentional_lstm.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_attentional_lstm_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),attentional_lstm.predict(X[train_idx]))\n",
    "        test_attentional_lstm_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),attentional_lstm.predict(X[test_idx]))\n",
    "        nine_train_attentional_lstm_auc = roc_auc_score(measured_affinity_less_than(y[nine_train_idx],500),attentional_lstm.predict(X[nine_train_idx]))\n",
    "        nine_test_attentional_lstm_auc = roc_auc_score(measured_affinity_less_than(y[nine_test_idx],500),attentional_lstm.predict(X[nine_test_idx]))\n",
    "        non_nine_train_attentional_lstm_auc = roc_auc_score(measured_affinity_less_than(y[non_nine_train_idx],500),attentional_lstm.predict(X[non_nine_train_idx]))\n",
    "        non_nine_test_attentional_lstm_auc = roc_auc_score(measured_affinity_less_than(y[non_nine_test_idx],500),attentional_lstm.predict(X[non_nine_test_idx]))\n",
    "        \n",
    "        train_attentional_lstm_aucs[i][epoch]=train_attentional_lstm_auc\n",
    "        test_attentional_lstm_aucs[i][epoch]=test_attentional_lstm_auc\n",
    "        nine_train_attentional_lstm_aucs[i][epoch]=nine_train_attentional_lstm_auc\n",
    "        nine_test_attentional_lstm_aucs[i][epoch]=nine_test_attentional_lstm_auc\n",
    "        non_nine_train_attentional_lstm_aucs[i][epoch]=non_nine_train_attentional_lstm_auc\n",
    "        non_nine_test_attentional_lstm_aucs[i][epoch]=non_nine_test_attentional_lstm_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
