Idea: 
* use LSTM only to learn long-term dependencies as opposed to local dependencies. 
* Do a step by step process where you filter out local dependencies of various radii and look at the performance. (very similar to fourier transforms)

# Step 1

* Take your 32 dimensional embedded amino acids for each amino acid in the sequence, and 32 dimensional average vector Z_0. 
* Feed that vector into the hidden layer of the shallow net and see how well you perform.

# Step 2

* take convolutional filter of size 2 and average the n-1 32 dimensional vectors, call this vector Z_{1}. n being the peptide length
* Take the flattened vector [Z_0,Z_1] and feed it into the shallow net and see how well you perform.

# Step 3

* take convolutional filter of size 3 and average the n-2 32 dimensional vectors, call this vector Z_{2}.
* Take the flattened vector [Z_0,Z_1,Z_2] and feed it into the shallow net and see how well you perform.
* keep going until you find reasonable k so that [Z_0,Z_1,...,Z_k] gives good performance. one of k=2,3,4 should be enough
* note that Z_8 gives something similar to a simple shallow net applications (with 9mer-index-encoding)

# Step 4

* So far we have learned local dependencies with different radii. 
* If there is still more complex long-term dependencies, we might use LSTM to complement what shallow nets cant learn. 
