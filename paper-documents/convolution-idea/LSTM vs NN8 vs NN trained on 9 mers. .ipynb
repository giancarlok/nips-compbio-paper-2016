{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn \n",
    "import scipy\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold, train_test_split, cross_val_score, StratifiedKFold, LabelKFold, ShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mhcflurry.amino_acid import common_amino_acids\n",
    "from mhcflurry import dataset\n",
    "from mhcflurry.dataset import Dataset\n",
    "import matplotlib.pyplot as plt \n",
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import math \n",
    "from mhcflurry import peptide_encoding, amino_acid\n",
    "import statsmodels.api as sm\n",
    "from keras import models, layers, optimizers\n",
    "from keras.optimizers import Adam \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Reshape, Embedding, LSTM, Input, merge, GlobalAveragePooling1D, Convolution1D, AveragePooling1D, Activation, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.engine import topology\n",
    "import seaborn as sns\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"bdata.2009.mhci.public.1.txt\")\n",
    "\n",
    "df['log_meas']=1-np.log(df['meas'])/math.log(50000)\n",
    "df['peptide_length'] = df['sequence'].str.len()\n",
    "\n",
    "\n",
    "max_len=df['sequence'].str.len().max()\n",
    "n_peptides = df['sequence'].count()\n",
    "\n",
    "def amino_acid_hotshot_encoding(s):\n",
    "    return common_amino_acids.hotshot_encoding([s],len(s)).flatten().astype(int)\n",
    "df['hotshot_encoded_peptides'] = df.sequence.apply(lambda seq: amino_acid_hotshot_encoding(seq))\n",
    "\n",
    "def amino_acid_index_encoding(s, maxlen):\n",
    "    a = 1+common_amino_acids.index_encoding([s],len(s)).flatten()\n",
    "    return np.concatenate([a, np.zeros(maxlen-len(a),dtype=int)])\n",
    "df['index_encoded_peptides'] = df.sequence.apply(lambda seq: amino_acid_index_encoding(seq, max_len))\n",
    "\n",
    "def new_index_encoding(x):\n",
    "    y = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        if x[i] != 0: \n",
    "            y[i] = x[i]+i*20\n",
    "    return y\n",
    "\n",
    "df['new_index_encoding'] = df.index_encoded_peptides.apply(lambda seq: new_index_encoding(seq))    \n",
    "def measured_affinity_less_than(Y,k):\n",
    "    IC50 = 50000**(1-Y)\n",
    "    return (IC50 < k).astype(int) \n",
    "\n",
    "def affinity_label(Y):\n",
    "    return measured_affinity_less_than(Y,50) + measured_affinity_less_than(Y,500) + measured_affinity_less_than(Y,5000) + measured_affinity_less_than(Y,50000)\n",
    "\n",
    "df['affinity_label'] = affinity_label(df['log_meas'])\n",
    "df_h = df[df['mhc']=='HLA-A-0201'][['hotshot_encoded_peptides','index_encoded_peptides','log_meas','peptide_length','new_index_encoding']]\n",
    "X = np.array(list(df_h['index_encoded_peptides']))\n",
    "X_new = np.array(list(df_h['new_index_encoding']))\n",
    "y = np.array(list(df_h['log_meas']))\n",
    "y[y<0]=0\n",
    "\n",
    "def first_and_last_three(Y):\n",
    "    k = np.count_nonzero(Y)\n",
    "    return np.concatenate([Y[:3],Y[-3+k:k]])\n",
    "def first_and_last_four(Y):\n",
    "    k = np.count_nonzero(Y)\n",
    "    return np.concatenate([Y[:4],Y[-4+k:k]])\n",
    "def first_and_last_two(Y):\n",
    "    k = np.count_nonzero(Y)\n",
    "    return np.concatenate([Y[:2],Y[-2+k:k]])\n",
    "def cut_the_zeros(Y):\n",
    "    k = np.count_nonzero(Y)\n",
    "    return Y[:k]\n",
    "\n",
    "X_44 = np.apply_along_axis(first_and_last_four,1,X)\n",
    "X_33 = np.apply_along_axis(first_and_last_three,1,X)\n",
    "X_22 = np.apply_along_axis(first_and_last_two,1,X)\n",
    "\n",
    "nine_mers = np.array(list(df_h[df_h['peptide_length']==9]['index_encoded_peptides']))\n",
    "nine_mers_new = np.array(list(df_h[df_h['peptide_length']==9]['new_index_encoding']))\n",
    "X_9 = np.apply_along_axis(cut_the_zeros,1,nine_mers)\n",
    "X_9_new = np.apply_along_axis(cut_the_zeros,1,nine_mers_new)\n",
    "y_9 = np.array(list(df_h[df_h['peptide_length']==9]['log_meas']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-- 2 3 4]\n",
      " [5 -- 7 8]\n",
      " [9 10 -- 12]]\n"
     ]
    }
   ],
   "source": [
    "def regroup_together(affinities, weights , original_indices):\n",
    "    affinities = affinities.ravel()\n",
    "    weights = weights.ravel()\n",
    "    \n",
    "    assert affinities.shape == weights.shape, \"%s should be %s\" % (affinities.shape, weights.shape)\n",
    "    assert affinities.shape == original_indices.shape\n",
    "    assert len(affinities) == len(affinities.ravel())\n",
    "    \n",
    "    weighted_affinities = (affinities * weights)\n",
    "    index_set = set(original_indices)\n",
    "    n_indices = len(index_set)\n",
    "    result_order = {original_index: i for (i, original_index) in enumerate(sorted(index_set))}\n",
    "    result = np.zeros(n_indices)\n",
    "    for i, x in enumerate(weighted_affinities):\n",
    "        result_idx = result_order[original_indices[i]]\n",
    "        result[result_idx] += x\n",
    "    return result\n",
    "\n",
    "def slicing(dataset, index, i):\n",
    "    return dataset.slice(index).kmer_index_encoding()[i]\n",
    "\n",
    "def label_transform(array):\n",
    "    result = 1-np.log(array)/math.log(50000)\n",
    "    result[result<0]=0\n",
    "    return result\n",
    "\n",
    "def index_to_hotshot_encoding(index_encoded_nine_mer):\n",
    "    result = np.zeros((9,21))\n",
    "    for position, amino_acid in enumerate(index_encoded_nine_mer):\n",
    "        result[position][amino_acid]= 1\n",
    "    return result.flatten()\n",
    "\n",
    "def real_labels(dataset,index):\n",
    "    \n",
    "    y = label_transform(slicing(dataset,index,1))\n",
    "    weights = slicing(dataset,index,2)\n",
    "    original_indices = slicing(dataset,index,3)\n",
    "    \n",
    "    return regroup_together(y, weights , original_indices)\n",
    "\n",
    "def fit(model,dataset,index, neural_network = False, hotshot = False): # to be left out or modified \n",
    "    \n",
    "    X = slicing(dataset,index,0)\n",
    "    \n",
    "    if (hotshot == True):\n",
    "        X = np.apply_along_axis(index_to_hotshot_encoding, 1, X)\n",
    "        \n",
    "    y = label_transform(slicing(dataset,index,1))\n",
    "    weights = slicing(dataset,index,2)\n",
    "    \n",
    "    if (neural_network == True):\n",
    "        model.fit(X, y, sample_weight = weights, batch_size = 16, nb_epoch = 1)\n",
    "    else: \n",
    "        model.fit(X, y, sample_weight = weights)\n",
    "        \n",
    "        \n",
    "def fit_new(model,dataset,index, neural_network = False):\n",
    "    X = slicing(dataset,index,0)\n",
    "    y = label_transform(slicing(dataset,index,1))\n",
    "    weights = slicing(dataset,index,2)\n",
    "    X_new = np.apply_along_axis(new_index_encoding, 1, X) \n",
    "    \n",
    "    if (neural_network == True):\n",
    "        model.fit(X_new, y, sample_weight = weights, batch_size = 16, nb_epoch = 1)\n",
    "    else: \n",
    "        model.fit(X_new, y, sample_weight = weights)\n",
    "        \n",
    "        \n",
    "def predict(model, dataset, index, hotshot = False):\n",
    "    \n",
    "    X = slicing(dataset,index,0)\n",
    "    \n",
    "    if (hotshot == True):\n",
    "        X = np.apply_along_axis(index_to_hotshot_encoding, 1, X)\n",
    "        \n",
    "    weights = slicing(dataset,index,2)\n",
    "    original_indices = slicing(dataset,index,3)\n",
    "    \n",
    "    return regroup_together(model.predict(X), weights , original_indices)\n",
    "\n",
    "def AUC(model, dataset, index, hotshot = False):\n",
    "        \n",
    "    real_affinity = measured_affinity_less_than(real_labels(dataset,index),500)\n",
    "    predicted_affinity = predict(model, dataset, index, hotshot = hotshot)\n",
    "    \n",
    "    return roc_auc_score(real_affinity, predicted_affinity)\n",
    "\n",
    "def AUC_simple(model, features, labels, index):\n",
    "    real_affinity = measured_affinity_less_than(labels[index],500)\n",
    "    predicted_affinity = model.predict(features[index])\n",
    "    \n",
    "    return roc_auc_score(real_affinity, predicted_affinity)\n",
    "\n",
    "def AUC_simple_average(model_1, model_2, features, labels, index):\n",
    "    real_affinity = measured_affinity_less_than(labels[index],500)\n",
    "    predicted_affinity_1 = model_1.predict(features[index])\n",
    "    predicted_affinity_2 = model_2.predict(features[index])\n",
    "    predicted_affinity = 0.5* predicted_affinity_1 + 0.5* predicted_affinity_2\n",
    "    \n",
    "    return roc_auc_score(real_affinity, predicted_affinity)\n",
    "def split_by_length(X,index,length=9):\n",
    "    length_idx = np.array([i for i in index if (np.count_nonzero(X[i])==length)])\n",
    "    non_length_idx = np.array([i for i in index if (np.count_nonzero(X[i])!=length)])\n",
    "    return index, length_idx, non_length_idx\n",
    "from numpy import ma \n",
    "\n",
    "array_test = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "position = [[1,0,0,0], [0,1,0,0], [0,0,1,0]]\n",
    "values = 0\n",
    "mx = ma.masked_array(array_test, mask=position, fill_value =0)\n",
    "print(mx)\n",
    "def sum_character_deletion(model,array,length): \n",
    "    result = np.zeros(len(array))\n",
    "    for i in range(length):\n",
    "        position_matrix = np.zeros((len(array),length))\n",
    "        position_matrix[:,i] = 1 \n",
    "        mx = ma.masked_array(array, mask=position_matrix)\n",
    "        result = result + model.predict(mx)\n",
    "        \n",
    "    return result / length\n",
    "        \n",
    "def random_dropout_prediction_by_lentgh(model,array,length):\n",
    "    array_of_lengths = np.apply_along_axis(np.count_nonzero,1,array)\n",
    "    #print(\"array shape\", array.shape, \"array of lengths shape\", array_of_lengths.shape)\n",
    "    bool_array = (array_of_lengths == length)\n",
    "    #print(\"bool_array shape\", bool_array.shape)\n",
    "    result = np.zeros(len(array))\n",
    "    #print(\"result shape\", result.shape)\n",
    "    for i in range(length):\n",
    "        position_matrix = np.zeros(array.shape)\n",
    "        position_matrix[:,i] = 1 \n",
    "        print(\"array shape\", array.shape, \"position_matrix shape\", position_matrix.shape)\n",
    "        mx = ma.masked_array(array, mask=position_matrix)\n",
    "        result[bool_array] = result[bool_array] + model.predict(mx)\n",
    "    \n",
    "    #print(\"result[bool_array] shape\", result[bool_array].shape, \"model prediction shape\", model.predict(array[bool_array]).shape)\n",
    "    return result/length\n",
    "\n",
    "def random_dropout_array_prediction(model,array):\n",
    "    array_of_lengths = np.apply_along_axis(np.count_nonzero,1,array)\n",
    "    result = np.zeros(len(array))\n",
    "    for length in np.unique(array_of_lengths):\n",
    "        result = result + random_dropout_prediction_by_lentgh(model,array,length)\n",
    "    return result\n",
    "\n",
    "def AUC_random_dropout(model, features, labels, index):\n",
    "    real_affinity = measured_affinity_less_than(labels[index],500)\n",
    "    predicted_affinity = random_dropout_array_prediction(model, features[index])\n",
    "    \n",
    "    return roc_auc_score(real_affinity, predicted_affinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0879     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0707     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.1030     \n",
      "test AUC : 0.907363101769 0.893691708956 0.93679707859 0 0\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0486     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0343     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0528     \n",
      "test AUC : 0.936524482238 0.938771259087 0.951031237142 0 1\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0386     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0299     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0410     \n",
      "test AUC : 0.94491924976 0.948635303799 0.954971025922 0 2\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0333     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0279     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0361     \n",
      "test AUC : 0.950903939789 0.953778631189 0.9578650048 0 3\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0305     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0267     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0336     \n",
      "test AUC : 0.954216671239 0.956483164175 0.960716979838 0 4\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0294     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 11s - loss: 0.0258    \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0318     \n",
      "test AUC : 0.956436017007 0.957497256892 0.960188931559 0 5\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0286     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0255     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0308     \n",
      "test AUC : 0.957434679742 0.958746228227 0.96082927582 0 6\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0283     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0252     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0296     \n",
      "test AUC : 0.957925010287 0.959239130435 0.960852420793 0 7\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0277     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0247     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0294     \n",
      "test AUC : 0.958897956385 0.959043683994 0.959831470306 0 8\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0271     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0252     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0283     \n",
      "test AUC : 0.959020539021 0.960052633384 0.960873851324 0 9\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0270     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 9s - loss: 0.0245     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0280     \n",
      "test AUC : 0.959168838294 0.960446097929 0.961134446578 0 10\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0263     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0244     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0276     \n",
      "test AUC : 0.959439720203 0.960712693732 0.96082927582 0 11\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0267     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0242     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0270     \n",
      "test AUC : 0.959308565355 0.960746982581 0.961149019339 0 12\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0269     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0242     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0273     \n",
      "test AUC : 0.959444006309 0.960767555891 0.960910711837 0 13\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0263     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0240     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0272     \n",
      "test AUC : 0.959666026608 0.960424667398 0.961043581127 0 14\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0262     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0235     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0266     \n",
      "test AUC : 0.959326567 0.960740124811 0.960190646002 0 15\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0259     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 12s - loss: 0.0235    \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 1s - loss: 0.0262     \n",
      "test AUC : 0.959557159512 0.960213790975 0.960415237965 0 16\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0262     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 17s - loss: 0.0234    \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0265     \n",
      "test AUC : 0.959825469757 0.960169215471 0.960313228638 0 17\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 9s - loss: 0.0260     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0234     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0259     \n",
      "test AUC : 0.959719174325 0.960243793718 0.960406665752 0 18\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0257     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 9s - loss: 0.0228     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0264     \n",
      "test AUC : 0.95945857907 0.960042346729 0.959999485667 0 19\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0251     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0229     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0257     \n",
      "test AUC : 0.959691743245 0.96058582499 0.960485530106 0 20\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0255     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0229     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0258     \n",
      "test AUC : 0.960179502126 0.960350946372 0.959628308874 0 21\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0254     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0226     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0258     \n",
      "test AUC : 0.959840042518 0.960522390619 0.959621451104 0 22\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0251     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 9s - loss: 0.0223     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0257     \n",
      "test AUC : 0.959731175422 0.960199218214 0.960287512001 0 23\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0249     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0220     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0254     \n",
      "test AUC : 0.959948909615 0.960538677822 0.959673741599 0 24\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0250     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 10s - loss: 0.0217    \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0251     \n",
      "test AUC : 0.959732889864 0.960488101769 0.959830613085 0 25\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0247     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 9s - loss: 0.0216     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0253     \n",
      "test AUC : 0.959768893156 0.959960053491 0.959798895899 0 26\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0240     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0214     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0252     \n",
      "test AUC : 0.959747462625 0.960003771773 0.959925764641 0 27\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0244     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 12s - loss: 0.0212    \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0253     \n",
      "test AUC : 0.960123782746 0.959696886572 0.960182931011 0 28\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0245     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0210     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0252     \n",
      "test AUC : 0.959616307777 0.959904334111 0.959365141956 0 29\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0240     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0205     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0253     \n",
      "test AUC : 0.960097208888 0.959933479632 0.959016252915 0 30\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0242     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0204     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0254     \n",
      "test AUC : 0.959811754218 0.960508675079 0.958665649431 0 31\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0234     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0205     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0246     \n",
      "test AUC : 0.959503154574 0.960255794816 0.959666883829 0 32\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0236     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0200     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0250     \n",
      "test AUC : 0.959647167741 0.960034631738 0.959203127143 0 33\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0234     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0198     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0247     \n",
      "test AUC : 0.959785180359 0.959677170484 0.959656597175 0 34\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0232     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 7s - loss: 0.0195     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0252     \n",
      "test AUC : 0.959115690577 0.959683171033 0.958966534083 0 35\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0234     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0192     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0249     \n",
      "test AUC : 0.959844328624 0.959565731724 0.959176553285 0 36\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0229     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0190     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0245     \n",
      "test AUC : 0.960006343437 0.95975774928 0.958384480867 0 37\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0231     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0188     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0246     \n",
      "test AUC : 0.960025202304 0.959689171581 0.959225414895 0 38\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0224     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0181     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0251     \n",
      "test AUC : 0.960089493897 0.959942480455 0.959198841037 0 39\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0221     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0179     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0246     \n",
      "test AUC : 0.959932622411 0.959822040872 0.958043306817 0 40\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0219     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0180     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0247     \n",
      "test AUC : 0.959564874503 0.959684028254 0.959974626252 0 41\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0220     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0174     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0242     \n",
      "test AUC : 0.95968831436 0.958981964065 0.957065217391 0 42\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0219     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0173     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0242     \n",
      "test AUC : 0.95999434234 0.959217271293 0.95792758195 0 43\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0214     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 8s - loss: 0.0169     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0245     \n",
      "test AUC : 0.959724317652 0.959314565903 0.958015018516 0 44\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0214     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0165     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0248     \n",
      "test AUC : 0.959902619668 0.958912529146 0.957450109724 0 45\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0212     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 6s - loss: 0.0158     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0242     \n",
      "test AUC : 0.960229220957 0.959546015636 0.958754800439 0 46\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0209     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0159     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0244     \n",
      "test AUC : 0.959420004115 0.958436771362 0.956880057605 0 47\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 3s - loss: 0.0206     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0156     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0244     \n",
      "test AUC : 0.960006343437 0.959496296804 0.957052359073 0 48\n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 4s - loss: 0.0210     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 5s - loss: 0.0151     \n",
      "Epoch 1/1\n",
      "4640/4640 [==============================] - 0s - loss: 0.0241     \n",
      "test AUC : 0.959880331916 0.959292278151 0.955923398711 0 49\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0882     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0735     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.1314     \n",
      "test AUC : 0.883834863603 0.875975634468 0.933020117111 1 0\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0522     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0351     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0642     \n",
      "test AUC : 0.929093996311 0.928507043435 0.949815529096 1 1\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0406     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0304     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0467     \n",
      "test AUC : 0.942489099447 0.945582830231 0.956258909106 1 2\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0337     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0285     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0401     \n",
      "test AUC : 0.948551658841 0.954095393817 0.959253067528 1 3\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0310     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0269     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0361     \n",
      "test AUC : 0.951949347364 0.957438928951 0.959943086534 1 4\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0289     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0263     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0341     \n",
      "test AUC : 0.954274449382 0.959091480798 0.960322160266 1 5\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0284     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0257     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0331     \n",
      "test AUC : 0.955777642685 0.959224243949 0.960650574375 1 6\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0282     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 9s - loss: 0.0253     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0308     \n",
      "test AUC : 0.957017930013 0.959889806585 0.961119612611 1 7\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0274     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0252     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0309     \n",
      "test AUC : 0.957285203197 0.960659308793 0.961323124546 1 8\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0267     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 11s - loss: 0.0247    \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0292     \n",
      "test AUC : 0.958071300799 0.960173675164 0.962001788809 1 9\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0269     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0245     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0289     \n",
      "test AUC : 0.958294901895 0.96048112667 0.961836708312 1 10\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0269     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 10s - loss: 0.0239    \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0287     \n",
      "test AUC : 0.958259090782 0.96074141232 0.961962483929 1 11\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0262     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 10s - loss: 0.0243    \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0282     \n",
      "test AUC : 0.959030339874 0.960261892783 0.962485675555 1 12\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0266     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0242     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0280     \n",
      "test AUC : 0.95868008972 0.961067206104 0.96298877802 1 13\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 9s - loss: 0.0264     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 12s - loss: 0.0236    \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0280     \n",
      "test AUC : 0.95886176561 0.960645333725 0.963114553636 1 14\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0259     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 12s - loss: 0.0235    \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0276     \n",
      "test AUC : 0.959623406842 0.960852339426 0.963663075074 1 15\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0256     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 9s - loss: 0.0232     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0276     \n",
      "test AUC : 0.95972035888 0.960602535077 0.962917155794 1 16\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0256     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0229     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0269     \n",
      "test AUC : 0.959555278383 0.962202680418 0.963892790262 1 17\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0254     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 9s - loss: 0.0235     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0274     \n",
      "test AUC : 0.96031429929 0.962176477165 0.963491007043 1 18\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0258     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0231     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0272     \n",
      "test AUC : 0.960272374085 0.961977332439 0.964243040416 1 19\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0252     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 9s - loss: 0.0229     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0265     \n",
      "test AUC : 0.959763157527 0.961147562748 0.96352856504 1 20\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0256     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0229     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0268     \n",
      "test AUC : 0.959831285986 0.96120258958 0.963918120074 1 21\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0251     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0226     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0262     \n",
      "test AUC : 0.960000733691 0.961725781206 0.963422878585 1 22\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0248     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0225     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0266     \n",
      "test AUC : 0.96005925429 0.961366796635 0.96260096987 1 23\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0247     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0223     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0265     \n",
      "test AUC : 0.960585939684 0.961658526189 0.963956551512 1 24\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0245     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0224     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0257     \n",
      "test AUC : 0.960242677064 0.96255205713 0.963214125999 1 25\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0249     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0219     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0257     \n",
      "test AUC : 0.960241803622 0.962502270949 0.964243040416 1 26\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0247     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0219     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0261     \n",
      "test AUC : 0.960371946448 0.962352912404 0.963394055006 1 27\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0238     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0216     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0260     \n",
      "test AUC : 0.960460164067 0.962020131086 0.962907547934 1 28\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0244     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0215     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0261     \n",
      "test AUC : 0.960433960814 0.962313607524 0.962975676393 1 29\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0242     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0213     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0258     \n",
      "test AUC : 0.960506456482 0.961726654648 0.962410559562 1 30\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0238     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0212     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0260     \n",
      "test AUC : 0.959914262955 0.962183464699 0.962842913243 1 31\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0236     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 10s - loss: 0.0208    \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0254     \n",
      "test AUC : 0.960190270557 0.962260327576 0.96190920398 1 32\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0236     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0208     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0256     \n",
      "test AUC : 0.96039290905 0.962004409134 0.962544196154 1 33\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0239     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 9s - loss: 0.0205     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0256     \n",
      "test AUC : 0.960348363519 0.962700542233 0.962480434904 1 34\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0237     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0204     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0258     \n",
      "test AUC : 0.960158826653 0.962552930572 0.961453267371 1 35\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0233     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0201     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0257     \n",
      "test AUC : 0.96018153614 0.961278579015 0.961510914528 1 36\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0235     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0198     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0257     \n",
      "test AUC : 0.960587686567 0.961901343004 0.960225208229 1 37\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0230     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0203     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0258     \n",
      "test AUC : 0.960800806361 0.961868152216 0.961568561686 1 38\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0226     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0193     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0253     \n",
      "test AUC : 0.960680271396 0.961994801275 0.96158515708 1 39\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0232     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 9s - loss: 0.0189     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0256     \n",
      "test AUC : 0.960254031807 0.962196566326 0.96045142965 1 40\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0225     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0186     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0251     \n",
      "test AUC : 0.959576240986 0.962157261446 0.959682800883 1 41\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0225     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0180     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0252     \n",
      "test AUC : 0.959799842082 0.961807884734 0.9604566703 1 42\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0221     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0185     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0250     \n",
      "test AUC : 0.959834779753 0.961993054391 0.960785084409 1 43\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0218     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0176     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0254     \n",
      "test AUC : 0.960640529795 0.961690843535 0.957839838728 1 44\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0219     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0172     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0251     \n",
      "test AUC : 0.959746562133 0.961994801275 0.959608558332 1 45\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0218     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0170     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0251     \n",
      "test AUC : 0.959947453743 0.962102234613 0.9578162558 1 46\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0218     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0165     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0250     \n",
      "test AUC : 0.960958899324 0.962310987199 0.958621569121 1 47\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0211     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0162     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0253     \n",
      "test AUC : 0.960712588742 0.961444532953 0.958326345799 1 48\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0210     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0159     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0247     \n",
      "test AUC : 0.959878451842 0.96202711862 0.955334807703 1 49\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0896     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0729     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.1452     \n",
      "test AUC : 0.86037573274 0.893183653116 0.931102095093 2 0\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0475     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0347     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0715     \n",
      "test AUC : 0.920327256568 0.940330886344 0.954178313613 2 1\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0369     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0299     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0526     \n",
      "test AUC : 0.937644173361 0.952142924989 0.957870847807 2 2\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0325     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0276     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0437     \n",
      "test AUC : 0.945136947731 0.956242536908 0.959514424121 2 3\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0306     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0265     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0406     \n",
      "test AUC : 0.949444338906 0.956675905069 0.958155802215 2 4\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0297     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0261     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0363     \n",
      "test AUC : 0.952794249349 0.959151446483 0.961573558945 2 5\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0288     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0257     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0350     \n",
      "test AUC : 0.95546993731 0.960103838743 0.960862869084 2 6\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0277     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0262     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0327     \n",
      "test AUC : 0.956321408218 0.960520245332 0.961688473594 2 7\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0277     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0253     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0316     \n",
      "test AUC : 0.957530768291 0.960233594768 0.960705974544 2 8\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0271     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0246     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0304     \n",
      "test AUC : 0.958296583261 0.960900184542 0.961665999512 2 9\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0267     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0249     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0300     \n",
      "test AUC : 0.958620549284 0.961448043313 0.960789934325 2 10\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0267     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0245     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0296     \n",
      "test AUC : 0.959561916522 0.960602508956 0.962708288102 2 11\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0267     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0242     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0291     \n",
      "test AUC : 0.95970778604 0.961904309596 0.961828830601 2 12\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0265     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0243     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0287     \n",
      "test AUC : 0.959739164948 0.962248629505 0.962760020897 2 13\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0260     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0238     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0284     \n",
      "test AUC : 0.959923198003 0.960032600141 0.96239619518 2 14\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0265     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0236     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0281     \n",
      "test AUC : 0.960455791359 0.961723668856 0.963403712549 2 15\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0254     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 8s - loss: 0.0237     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0279     \n",
      "test AUC : 0.96072632843 0.961821197894 0.96334265089 2 16\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0257     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0233     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0274     \n",
      "test AUC : 0.960886615284 0.961805084401 0.962115481166 2 17\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0253     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0233     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0277     \n",
      "test AUC : 0.960672899479 0.961398854755 0.96238432208 2 18\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0254     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0232     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0273     \n",
      "test AUC : 0.960751770788 0.961635468682 0.962317323871 2 19\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0256     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 9s - loss: 0.0229     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0271     \n",
      "test AUC : 0.961350514275 0.962106152301 0.961216517857 2 20\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0251     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0228     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0266     \n",
      "test AUC : 0.961415816327 0.962205377497 0.962497964611 2 21\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0253     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0229     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0271     \n",
      "test AUC : 0.961021459781 0.962138379288 0.961978092434 2 22\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0249     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0223     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0270     \n",
      "test AUC : 0.960844211355 0.961227542879 0.962741363168 2 23\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0251     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0224     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0273     \n",
      "test AUC : 0.961017219388 0.962152796624 0.961210581307 2 24\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0248     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0222     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0262     \n",
      "test AUC : 0.961299629559 0.96201625597 0.963006811767 2 25\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0249     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0219     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0264     \n",
      "test AUC : 0.961224998643 0.962236756405 0.961634620604 2 26\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0243     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0218     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0271     \n",
      "test AUC : 0.961394614362 0.962235060248 0.9617813382 2 27\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0243     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0217     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0269     \n",
      "test AUC : 0.961474333749 0.962024736756 0.962360575879 2 28\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0244     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0216     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0263     \n",
      "test AUC : 0.961532003094 0.962459801075 0.960505827996 2 29\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0246     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 6s - loss: 0.0218     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0260     \n",
      "test AUC : 0.961723668856 0.962230819855 0.96078060546 2 30\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 3s - loss: 0.0237     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0211     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0263     \n",
      "test AUC : 0.96194416929 0.962265591077 0.960763643888 2 31\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0239     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0209     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0263     \n",
      "test AUC : 0.961640557154 0.961757592 0.961425145191 2 32\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0235     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0210     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0262     \n",
      "test AUC : 0.961716884227 0.962636201422 0.960979055851 2 33\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0232     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0210     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0266     \n",
      "test AUC : 0.962049331036 0.962493724218 0.960665266772 2 34\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0236     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0204     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0261     \n",
      "test AUC : 0.962076469551 0.961876323003 0.960913753799 2 35\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0231     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0202     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0261     \n",
      "test AUC : 0.961873778767 0.962366512429 0.95888175749 2 36\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0227     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0199     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0263     \n",
      "test AUC : 0.961784730515 0.962473370332 0.960596572406 2 37\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0228     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0196     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0263     \n",
      "test AUC : 0.96167872069 0.9620349137 0.960487170267 2 38\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0233     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0194     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0255     \n",
      "test AUC : 0.961547268508 0.961979788591 0.96037692005 2 39\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 5s - loss: 0.0220     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0192     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0265     \n",
      "test AUC : 0.961581191652 0.96210106383 0.960309073762 2 40\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0222     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0189     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0252     \n",
      "test AUC : 0.961704163048 0.962395347102 0.960626255156 2 41\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0219     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0188     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0262     \n",
      "test AUC : 0.961599849381 0.961758440078 0.959963057696 2 42\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0223     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0185     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0258     \n",
      "test AUC : 0.961773705493 0.961562533923 0.959431312419 2 43\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0217     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0181     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0257     \n",
      "test AUC : 0.961726213092 0.961914486539 0.95888769404 2 44\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0212     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0180     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0257     \n",
      "test AUC : 0.961788122829 0.961904309596 0.958196509987 2 45\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0215     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0174     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0260     \n",
      "test AUC : 0.961931448111 0.962207921733 0.957821659249 2 46\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0210     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0174     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0256     \n",
      "test AUC : 0.961657942765 0.962062900293 0.959321062201 2 47\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0202     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0170     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0260     \n",
      "test AUC : 0.961545572351 0.962224883304 0.959480500977 2 48\n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 4s - loss: 0.0205     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 7s - loss: 0.0164     \n",
      "Epoch 1/1\n",
      "4641/4641 [==============================] - 0s - loss: 0.0260     \n",
      "test AUC : 0.961810172872 0.962006927106 0.958688395571 2 49\n"
     ]
    }
   ],
   "source": [
    "folds = 3\n",
    "batch_size_nn = 16\n",
    "batch_size_lstm = 16\n",
    "hidden = 50\n",
    "dropout_probability = 0.25\n",
    "\n",
    "n_epochs = 50\n",
    "epoch = 0\n",
    "\n",
    "\n",
    "\n",
    "nn_8_aucs = np.zeros((2, folds,n_epochs))\n",
    "lstm_aucs = np.zeros((2, folds,n_epochs))\n",
    "nn_aucs = np.zeros((2, folds,n_epochs))\n",
    "\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(KFold(len(X_9),folds, shuffle=True)):\n",
    "    \n",
    "    list_index = train_idx, test_idx\n",
    "    \n",
    "    sequence = Input( shape= (9, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 9, output_dim= 32)(sequence)\n",
    "    \n",
    "    z_0 = GlobalAveragePooling1D()(embedded)\n",
    "    z_1 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (26, 32), filter_length = 2, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_2 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (26, 32), filter_length = 3, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_3 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (26, 32), filter_length = 4, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_4 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (26, 32), filter_length = 5, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_5 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (26, 32), filter_length = 6, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_6 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (26, 32), filter_length = 7, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_7 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (26, 32), filter_length = 8, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_8 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (26, 32), filter_length = 9, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    merged = merge([z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,], mode = 'concat', concat_axis=-1)\n",
    "    z = Reshape((9,32)) (merged)\n",
    "    lstm = LSTM(hidden)(z)\n",
    "    \n",
    "    after_dp = Dropout(dropout_probability)(lstm)\n",
    "    output = Dense(1, init='glorot_uniform', activation = 'sigmoid')(after_dp)\n",
    "    lstm = Model(input = sequence, output = output)\n",
    "    lstm.compile(optimizer = 'adam', loss='mean_squared_error')\n",
    "\n",
    "    # nn_8    \n",
    "    sequence = Input( shape= (9, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 9, output_dim= 32)(sequence)\n",
    "    \n",
    "    z_0 = GlobalAveragePooling1D()(embedded)\n",
    "    z_1 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (9, 32), filter_length = 2, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_2 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (9, 32), filter_length = 3, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_3 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (9, 32), filter_length = 4, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_4 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (9, 32), filter_length = 5, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_5 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (9, 32), filter_length = 6, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_6 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (9, 32), filter_length = 7, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_7 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (9, 32), filter_length = 8, border_mode='valid', activation='sigmoid')(embedded))\n",
    "    z_8 = GlobalAveragePooling1D()(Convolution1D(nb_filter = 32, input_shape = (9, 32), filter_length = 9, border_mode='valid', activation='sigmoid')(embedded))\n",
    "\n",
    "    z = merge([z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8], mode = 'concat', concat_axis=-1)\n",
    "    hidden_layer = Dense(10, init='glorot_uniform', activation = 'sigmoid')(z)\n",
    "    \n",
    "    \n",
    "    after_dp = Dropout(dropout_probability)(hidden_layer)\n",
    "    output = Dense(1, init='glorot_uniform', activation = 'sigmoid')(after_dp)\n",
    "    nn_8 = Model(input = sequence, output = output)\n",
    "    nn_8.compile(optimizer = 'adam', loss='mean_squared_error')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #nn\n",
    "    sequence = Input( shape= (9, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 9, output_dim= 32)(sequence)\n",
    "    \n",
    "    z = Flatten()(embedded)\n",
    "    \n",
    "    hidden_layer = Dense(10, init='glorot_uniform', activation = 'sigmoid')(z)\n",
    "    \n",
    "    after_dp = Dropout(dropout_probability)(hidden_layer)\n",
    "    output = Dense(1, init='glorot_uniform', activation = 'sigmoid')(after_dp)\n",
    "    nn = Model(input = sequence, output = output)\n",
    "    nn.compile(optimizer = 'adam', loss='mean_squared_error')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        \n",
    "        #nn\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "        #deep_nn\n",
    "        nn_8.fit(X_9[train_idx],y_9[train_idx], batch_size = 16, nb_epoch = 1)\n",
    "        for k, index in enumerate([train_idx,test_idx]):\n",
    "            nn_8_aucs[k][i][epoch] = AUC_simple(nn_8, X_9, y_9, index)\n",
    "        \n",
    "        lstm.fit(X_9[train_idx],y_9[train_idx], batch_size = 16, nb_epoch = 1)\n",
    "        for k, index in enumerate([train_idx,test_idx]):\n",
    "            lstm_aucs[k][i][epoch] = AUC_simple(lstm, X_9, y_9, index)\n",
    "            \n",
    "            \n",
    "        nn.fit(X_9[train_idx],y_9[train_idx], batch_size = 16, nb_epoch = 1)\n",
    "        for k, index in enumerate([train_idx,test_idx]):\n",
    "            nn_aucs[k][i][epoch] = AUC_simple(nn, X_9, y_9, index)\n",
    " \n",
    "    \n",
    "        \n",
    "        print(\"test AUC :\",  nn_aucs[1][i][epoch], nn_8_aucs[1][i][epoch],lstm_aucs[1][i][epoch] , i, epoch)  \n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhEAAAFhCAYAAADOVPhOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAeLALwJfY6wBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13d8bce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( np.arange(0,n_epochs,1), nn_aucs[1,:,:].mean(axis=0),color='r',marker='*', linestyle='-', label =\"test NN\")\n",
    "plt.plot( np.arange(0,n_epochs,1), nn_8_aucs[1,:,:].mean(axis=0),color='b',marker='*', linestyle='-', label =\"test NN_8 \")\n",
    "plt.plot( np.arange(0,n_epochs,1), lstm_aucs[1,:,:].mean(axis=0),color='navy',marker='*', linestyle='-', label =\"test LSTM \")\n",
    "plt.legend( loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
