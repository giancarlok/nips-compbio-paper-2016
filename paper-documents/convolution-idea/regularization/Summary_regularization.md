# Learning rates (with 1/epoch decay)

![](https://github.com/giancarlok/nips-compbio-paper-2016/blob/master/paper-documents/convolution-idea/regularization/LR%20exploration%20.png)




# Dropout regularization analysis 

![](https://github.com/giancarlok/nips-compbio-paper-2016/blob/master/paper-documents/convolution-idea/regularization/LSTM%20dropout%20regularity.png)

W dropout seems to be the right parameter to tune


# W dropout 

Here we set W dropout to 0.5 in the hope to see even stronger effeect than above. 

![](https://github.com/giancarlok/nips-compbio-paper-2016/blob/master/paper-documents/convolution-idea/regularization/W%20regularization.png)
