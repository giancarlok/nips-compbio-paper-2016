# Learning rates (with 1/epoch decay) [notebook](https://github.com/giancarlok/nips-compbio-paper-2016/blob/master/paper-documents/convolution-idea/regularization/LSTM%20LR%20discovery.ipynb)

![](https://github.com/giancarlok/nips-compbio-paper-2016/blob/master/paper-documents/convolution-idea/regularization/LR%20exploration%20.png)




# Dropout regularization analysis [notebook](https://github.com/giancarlok/nips-compbio-paper-2016/blob/master/paper-documents/convolution-idea/regularization/LSTM9%20vs%20LSTM9_U%20vs%20LSTM9_W%20vs%20NN.ipynb)

![](https://github.com/giancarlok/nips-compbio-paper-2016/blob/master/paper-documents/convolution-idea/regularization/LSTM%20dropout%20regularity.png)

W dropout seems to be the right parameter to tune


# W dropout [notebook](https://github.com/giancarlok/nips-compbio-paper-2016/blob/master/paper-documents/convolution-idea/regularization/LSTMW%20analysis%20with%20W%20%3D%200.5%20part%201.ipynb)

Here we set W dropout to 0.5 in the hope to see even stronger effeect than above. 

![](https://github.com/giancarlok/nips-compbio-paper-2016/blob/master/paper-documents/convolution-idea/regularization/W%20regularization.png)
