{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn \n",
    "import scipy\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold, train_test_split, cross_val_score, StratifiedKFold, LabelKFold, ShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mhcflurry.amino_acid import common_amino_acids\n",
    "from mhcflurry import dataset\n",
    "from mhcflurry.dataset import Dataset\n",
    "import matplotlib.pyplot as plt \n",
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import math \n",
    "from mhcflurry import peptide_encoding, amino_acid\n",
    "import statsmodels.api as sm\n",
    "from keras import models, layers, optimizers\n",
    "from keras.optimizers import Adam \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, Convolution1D, AveragePooling1D, Activation, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.engine import topology\n",
    "import seaborn as sns\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_csv(\"bdata.2009.mhci.public.1.txt\")\n",
    "ds_h = ds.slice(ds.alleles == 'HLA-A0201')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"bdata.2009.mhci.public.1.txt\")\n",
    "\n",
    "df['log_meas']=1-np.log(df['meas'])/math.log(50000)\n",
    "df['peptide_length'] = df['sequence'].str.len()\n",
    "\n",
    "\n",
    "max_len=df['sequence'].str.len().max()\n",
    "n_peptides = df['sequence'].count()\n",
    "\n",
    "def amino_acid_hotshot_encoding(s):\n",
    "    return common_amino_acids.hotshot_encoding([s],len(s)).flatten().astype(int)\n",
    "df['hotshot_encoded_peptides'] = df.sequence.apply(lambda seq: amino_acid_hotshot_encoding(seq))\n",
    "\n",
    "def amino_acid_index_encoding(s, maxlen):\n",
    "    a = 1+common_amino_acids.index_encoding([s],len(s)).flatten()\n",
    "    return np.concatenate([a, np.zeros(maxlen-len(a),dtype=int)])\n",
    "df['index_encoded_peptides'] = df.sequence.apply(lambda seq: amino_acid_index_encoding(seq, max_len))\n",
    "\n",
    "def measured_affinity_less_than(Y,k):\n",
    "    IC50 = 50000**(1-Y)\n",
    "    return (IC50 < k).astype(int) \n",
    "\n",
    "def affinity_label(Y):\n",
    "    return measured_affinity_less_than(Y,50) + measured_affinity_less_than(Y,500) + measured_affinity_less_than(Y,5000) + measured_affinity_less_than(Y,50000)\n",
    "\n",
    "df['affinity_label'] = affinity_label(df['log_meas'])\n",
    "df_h = df[df['mhc']=='HLA-A-0201'][['hotshot_encoded_peptides','index_encoded_peptides','log_meas','peptide_length']]\n",
    "X = np.array(list(df_h['index_encoded_peptides']))\n",
    "y = np.array(list(df_h['log_meas']))\n",
    "y[y<0]=0\n",
    "\n",
    "def first_and_last_three(Y):\n",
    "    k = np.count_nonzero(Y)\n",
    "    return np.concatenate([Y[:3],Y[-3+k:k]])\n",
    "def first_and_last_four(Y):\n",
    "    k = np.count_nonzero(Y)\n",
    "    return np.concatenate([Y[:4],Y[-4+k:k]])\n",
    "def first_and_last_two(Y):\n",
    "    k = np.count_nonzero(Y)\n",
    "    return np.concatenate([Y[:2],Y[-2+k:k]])\n",
    "X_44 = np.apply_along_axis(first_and_last_four,1,X)\n",
    "X_33 = np.apply_along_axis(first_and_last_three,1,X)\n",
    "X_22 = np.apply_along_axis(first_and_last_two,1,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regroup_together(affinities, weights , original_indices):\n",
    "    affinities = affinities.ravel()\n",
    "    weights = weights.ravel()\n",
    "    \n",
    "    assert affinities.shape == weights.shape, \"%s should be %s\" % (affinities.shape, weights.shape)\n",
    "    assert affinities.shape == original_indices.shape\n",
    "    assert len(affinities) == len(affinities.ravel())\n",
    "    \n",
    "    weighted_affinities = (affinities * weights)\n",
    "    index_set = set(original_indices)\n",
    "    n_indices = len(index_set)\n",
    "    result_order = {original_index: i for (i, original_index) in enumerate(sorted(index_set))}\n",
    "    result = np.zeros(n_indices)\n",
    "    for i, x in enumerate(weighted_affinities):\n",
    "        result_idx = result_order[original_indices[i]]\n",
    "        result[result_idx] += x\n",
    "    return result\n",
    "\n",
    "def slicing(dataset, index, i):\n",
    "    return dataset.slice(index).kmer_index_encoding()[i]\n",
    "\n",
    "def label_transform(array):\n",
    "    result = 1-np.log(array)/math.log(50000)\n",
    "    result[result<0]=0\n",
    "    return result\n",
    "\n",
    "def index_to_hotshot_encoding(index_encoded_nine_mer):\n",
    "    result = np.zeros((9,21))\n",
    "    for position, amino_acid in enumerate(index_encoded_nine_mer):\n",
    "        result[position][amino_acid]= 1\n",
    "    return result.flatten()\n",
    "\n",
    "def real_labels(dataset,index):\n",
    "    \n",
    "    y = label_transform(slicing(dataset,index,1))\n",
    "    weights = slicing(dataset,index,2)\n",
    "    original_indices = slicing(dataset,index,3)\n",
    "    \n",
    "    return regroup_together(y, weights , original_indices)\n",
    "\n",
    "def fit(model,dataset,index, neural_network = False, hotshot = False): # to be left out or modified \n",
    "    \n",
    "    X = slicing(dataset,index,0)\n",
    "    \n",
    "    if (hotshot == True):\n",
    "        X = np.apply_along_axis(index_to_hotshot_encoding, 1, X)\n",
    "        \n",
    "    y = label_transform(slicing(dataset,index,1))\n",
    "    weights = slicing(dataset,index,2)\n",
    "    \n",
    "    if (neural_network == True):\n",
    "        model.fit(X, y, sample_weight = weights, batch_size = 16, nb_epoch = 1)\n",
    "    else: \n",
    "        model.fit(X, y, sample_weight = weights)\n",
    "        \n",
    "def predict(model, dataset, index, hotshot = False):\n",
    "    \n",
    "    X = slicing(dataset,index,0)\n",
    "    \n",
    "    if (hotshot == True):\n",
    "        X = np.apply_along_axis(index_to_hotshot_encoding, 1, X)\n",
    "        \n",
    "    weights = slicing(dataset,index,2)\n",
    "    original_indices = slicing(dataset,index,3)\n",
    "    \n",
    "    return regroup_together(model.predict(X), weights , original_indices)\n",
    "\n",
    "def AUC(model, dataset, index, hotshot = False):\n",
    "        \n",
    "    real_affinity = measured_affinity_less_than(real_labels(dataset,index),500)\n",
    "    predicted_affinity = predict(model, dataset, index, hotshot = hotshot)\n",
    "    \n",
    "    return roc_auc_score(real_affinity, predicted_affinity)\n",
    "\n",
    "def AUC_simple(model, features, labels, index):\n",
    "    real_affinity = measured_affinity_less_than(labels[index],500)\n",
    "    predicted_affinity = model.predict(features[index])\n",
    "    \n",
    "    return roc_auc_score(real_affinity, predicted_affinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_by_length(X,index,length=9):\n",
    "    length_idx = np.array([i for i in index if (np.count_nonzero(X[i])==length)])\n",
    "    non_length_idx = np.array([i for i in index if (np.count_nonzero(X[i])!=length)])\n",
    "    return index, length_idx, non_length_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_dropout_prediction_by_lentgh(model,array,length):\n",
    "    array_of_lengths = np.apply_along_axis(np.count_nonzero,1,array)\n",
    "    #print(\"array shape\", array.shape, \"array of lengths shape\", array_of_lengths.shape)\n",
    "    bool_array = (array_of_lengths == length)\n",
    "    #print(\"bool_array shape\", bool_array.shape)\n",
    "    result = np.zeros(len(array))\n",
    "    #print(\"result shape\", result.shape)\n",
    "    for i in range(length):\n",
    "        result[bool_array] = result[bool_array] + model.predict(array[bool_array])[:,0]\n",
    "    #print(\"result[bool_array] shape\", result[bool_array].shape, \"model prediction shape\", model.predict(array[bool_array]).shape)\n",
    "    return result/length\n",
    "\n",
    "def random_dropout_array_prediction(model,array):\n",
    "    array_of_lengths = np.apply_along_axis(np.count_nonzero,1,array)\n",
    "    result = np.zeros(len(array))\n",
    "    for length in np.unique(array_of_lengths):\n",
    "        result = result + random_dropout_prediction_by_lentgh(model,array,length)\n",
    "    return result\n",
    "\n",
    "def AUC_random_dropout(model, features, labels, index):\n",
    "    real_affinity = measured_affinity_less_than(labels[index],500)\n",
    "    predicted_affinity = random_dropout_array_prediction(model, features[index])\n",
    "    \n",
    "    return roc_auc_score(real_affinity, predicted_affinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0472    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0445    \n",
      "allmers: 0.934507891983 0.931545225851 0 0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0304    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0276    \n",
      "allmers: 0.939346824284 0.944147910684 0 1\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0272    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0244    \n",
      "allmers: 0.940943476035 0.944200449698 0 2\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0258    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0232    \n",
      "allmers: 0.942226229435 0.945443576215 0 3\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0247    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0226    \n",
      "allmers: 0.942288563859 0.945269039827 0 4\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0246    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0220    \n",
      "allmers: 0.942443954674 0.94529041163 0 5\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0240    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0220    \n",
      "allmers: 0.943298826777 0.9455588949 0 6\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0240    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 10s - loss: 0.0218    \n",
      "allmers: 0.942882076627 0.945527727688 0 7\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0240     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0213     \n",
      "allmers: 0.94249204123 0.945474743427 0 8\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0235     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0213     \n",
      "allmers: 0.942924820232 0.945513034573 0 9\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0236     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0213     \n",
      "allmers: 0.942998731049 0.945631469979 0 10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0235     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0211     \n",
      "allmers: 0.94318484383 0.94568445424 0 11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0232     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0212     \n",
      "allmers: 0.94377479463 0.945588726374 0 12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0235     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0211     \n",
      "allmers: 0.943605155947 0.945576704735 0 13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0235     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0212     \n",
      "allmers: 0.94336561366 0.945661301454 0 14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0232     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0211    \n",
      "allmers: 0.943114940226 0.945694472272 0 15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0235    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0208     \n",
      "allmers: 0.943001402525 0.945656403749 0 16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0230    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0209     \n",
      "allmers: 0.942388298938 0.945657294241 0 17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0232     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0209     \n",
      "allmers: 0.943547719228 0.945641933258 0 18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0233     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0208     \n",
      "allmers: 0.942738707451 0.945691132928 0 19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0232     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0207     \n",
      "allmers: 0.944006767737 0.945725862107 0 20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0233     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0207     \n",
      "allmers: 0.943150559897 0.945710723747 0 21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0231     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0209     \n",
      "allmers: 0.943609608406 0.945749014893 0 22\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0232     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0210     \n",
      "allmers: 0.94304948908 0.945751241123 0 23\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0230     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0206     \n",
      "allmers: 0.94393508315 0.945774839155 0 24\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0231     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0208     \n",
      "allmers: 0.944442218215 0.945750350631 0 25\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0231     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0207     \n",
      "allmers: 0.943323760547 0.945762817516 0 26\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0228     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0204     \n",
      "allmers: 0.944022351344 0.945756138828 0 27\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0229     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0209     \n",
      "allmers: 0.943345132349 0.945769496204 0 28\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0230     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0207     \n",
      "allmers: 0.942434159264 0.945787751286 0 29\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0227     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0205     \n",
      "allmers: 0.943654578241 0.945776620138 0 30\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0229     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0210     \n",
      "allmers: 0.943320643826 0.945783298827 0 31\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0227     \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0206     \n",
      "allmers: 0.943486275296 0.94580021817 0 32\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0229    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0208    \n",
      "allmers: 0.943361161201 0.945782853581 0 33\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0229    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0205    \n",
      "allmers: 0.943776130368 0.945785525056 0 34\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0228    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0207    \n",
      "allmers: 0.943806852334 0.945785970302 0 35\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0230    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0207    \n",
      "allmers: 0.942964447116 0.945786415548 0 36\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0230    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0203    \n",
      "allmers: 0.943074868096 0.945797101449 0 37\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0226    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0204    \n",
      "allmers: 0.943563302834 0.945797991941 0 38\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0226    \n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0207    \n"
     ]
    }
   ],
   "source": [
    "folds = 3\n",
    "batch_size_nn = 16\n",
    "batch_size_lstm = 16\n",
    "hidden = 50\n",
    "dropout_probability = 0.25\n",
    "\n",
    "n_epochs = 40\n",
    "epoch = 0\n",
    "\n",
    "\n",
    "\n",
    "lstm_avg_aucs = np.zeros((6, folds,n_epochs))\n",
    "lstm_aucs = np.zeros((6, folds,n_epochs))\n",
    "\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(KFold(len(df_h),folds, shuffle=True)):\n",
    "    \n",
    "    list_index = split_by_length(X,train_idx,length=9)+split_by_length(X,test_idx,length=9)\n",
    "    \n",
    "    \n",
    "    # lstm_avg \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Lambda(lambda x: K.dropout(x, level=0.5))(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_avg = Model(input = sequence, output = output)\n",
    "    adam_avg = Adam(lr = 0.01)\n",
    "    lstm_avg.compile(optimizer = adam_avg, loss='mean_squared_error')\n",
    "    \n",
    "    # lstm \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm = Model(input = sequence, output = output)\n",
    "    adam = Adam(lr = 0.01)\n",
    "    lstm.compile(optimizer = adam, loss='mean_squared_error')\n",
    "      \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        \n",
    "        #lstm_avg \n",
    "        adam_avg.lr.set_value(0.01*(1+epoch)**(-2))\n",
    "        lstm_avg.fit(X[train_idx],y[train_idx], batch_size = 16, nb_epoch = 1)\n",
    "        for k, index in enumerate(list_index):\n",
    "            lstm_avg_aucs[k][i][epoch] = AUC_random_dropout(lstm_avg, X, y, index)\n",
    "            \n",
    "        # lstm    \n",
    "        adam.lr.set_value(0.01*(1+epoch)**(-2))\n",
    "        lstm.fit(X[train_idx],y[train_idx], batch_size = 16, nb_epoch = 1)\n",
    "        for k, index in enumerate(list_index):\n",
    "            lstm_aucs[k][i][epoch] = AUC_random_dropout(lstm, X, y, index)\n",
    "        \n",
    "        print(\"allmers:\", lstm_avg_aucs[3][i][epoch], lstm_aucs[3][i][epoch], i, epoch)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot( np.arange(0,40,1), nn_aucs[3,0,0:40],color='r',marker='*', linestyle='-', label =\"test NN \")\n",
    "plt.plot( np.arange(0,40,1), lstm_avg_aucs[3,0,0:40],color='g',marker='*', linestyle='-', label =\"test LSTM \")\n",
    "plt.plot( np.arange(0,40,1), kmer_lstm_aucs[3,0,0:40],color='b',marker='*', linestyle='-', label =\"test kemr LSTM\")\n",
    "plt.plot( np.arange(0,40,1), nn_44_aucs[3,0,0:40],color='orange',marker='*', linestyle='-', label =\"test NN44\")\n",
    "plt.plot( np.arange(0,40,1), lstm_44_aucs[3,0,0:40],color='y',marker='*', linestyle='-', label =\"test LSTM44\")\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, 16, 16, 17,  7, 15,  9, 18,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,None][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, 16, 16, 17,  7, 15,  9, 18,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ravel(a[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=X[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  5,  4,  5,  8, 12, 16, 10, 10,  9,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2356"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.apply_along_axis(np.count_nonzero,1,X) == 10).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i in np.unique(np.apply_along_axis(np.count_nonzero,1,X)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
